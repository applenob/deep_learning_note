{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度生成模型总结--原始GAN和VAE\n",
    "\n",
    "前段时间研究了一段`Zero Shot Learning`，研究了一些生成模型，总结一下备忘。\n",
    "\n",
    "关于生成模型有一个很好的[github仓库](https://github.com/wiseodd/generative-models)，而且作者的[博客](https://wiseodd.github.io/)写的也非常精彩。\n",
    "\n",
    "所谓生成模型，即，给定一些数据，用一个向量$x$表示，每一个datapoint对应一张图片或者一句话，生成模型的目标是学会$P(x)$，有了$P(x)$就可以从中sample，从而**生成**一些数据。\n",
    "\n",
    "本文主要总结下**GAN**和**VAE**两种生成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "GAN的原始paper是大神Goodfellow的[《Generative Adversarial Nets》](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n",
    "\n",
    "先简介下模型的**思想**：\n",
    "\n",
    "GAN包含：**一个生成模型$G$**和**一个判别模型$D$**，$G$的输入是一个随机扰动$z$，输出是生成的数据$\\hat x$；$D$的输入是原始数据$x$或者$G$生成的数据$\\hat x$，输出是**输入来自训练数据的概率**。\n",
    "\n",
    "如果将$G$看作假币生产者，$D$看作警察，那么作假者的任务是尽量生产可以以假换真的假币，警察的任务是尽可能区分假币和真币，二者将不断地竞争直到生产者生产的假币完全以假乱真。\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/gan_1.png)\n",
    "\n",
    "**目标函数**：\n",
    "$$\\underset{G}{min}\\underset{D}{max}\\;V(D, G) = E_{x∼p_{data}(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 − D(G(z)))]$$\n",
    "\n",
    "上面的目标函数修改自**交叉熵**损失函数。\n",
    "\n",
    "很多初次接触GAN的同学都会对GAN的训练感到迷惑，看下面的**训练算法**：\n",
    "\n",
    "- 每轮训练迭代：\n",
    "    - 前k步，训练分类器：\n",
    "        - 从先验噪声：$p_g(z)$中采样m个**噪声**作为minibatch：$\\{z^{(1)}, ..., z^{(m)}\\}$。\n",
    "        - 从数据生成分布中$p_{data}(x)$采样m个**真实数据**作为minibatch：$\\{x^{(1)}, ..., x^{(m)}\\}$。\n",
    "        - 使用随机梯度**上升**更新**判别器的参数**：\n",
    "            - $\\bigtriangledown _{\\theta_d}\\frac{1}{m}\\sum_{i=1}^m[logD(x^{(i)})+log(D(G(z^{(i)})))]$\n",
    "    - 从先验噪声：$p_g(z)$中采样m个**噪声**作为minibatch：$\\{z^{(1)}, ..., z^{(m)}\\}$。\n",
    "    - 使用随机梯度**下降**更新**生成器的参数**：\n",
    "        - $\\bigtriangledown _{\\theta_g}\\frac{1}{m}\\sum_{i=1}^mlog(D(G(z^{(i)})))$\n",
    "        \n",
    "虽然是同一个损失函数按照相反的方向训练，但训练判别器和生成器时针对的**参数**不同（上面算法中$\\theta_d$和$\\theta_g$），因此并不会出现南辕北辙的情况。\n",
    "\n",
    "**优缺点**:\n",
    "- 缺点：没有明确的$p_g(x)$的表示；$D$的训练需要和$G$同步。\n",
    "- 优点：采样时无需Markov chain；训练时无需inference，模型设计的范围更广。\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/gan_cmp.png)\n",
    "\n",
    "下面代码来自https://github.com/wiseodd/generative-models ，使用python3.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cer/anaconda2/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: [ 1.50280118]; G_loss: [ 1.64008629]\n",
      "Iter-1000; D_loss: [ 0.0023118]; G_loss: [ 8.38419437]\n",
      "Iter-2000; D_loss: [ 0.00522094]; G_loss: [ 7.93807125]\n",
      "Iter-3000; D_loss: [ 0.03023708]; G_loss: [ 4.29144192]\n",
      "Iter-4000; D_loss: [ 0.02870584]; G_loss: [ 6.23502254]\n",
      "Iter-5000; D_loss: [ 0.15886861]; G_loss: [ 4.66118479]\n",
      "Iter-6000; D_loss: [ 0.26347995]; G_loss: [ 4.20204782]\n",
      "Iter-7000; D_loss: [ 0.51319796]; G_loss: [ 2.98293495]\n",
      "Iter-8000; D_loss: [ 0.70317507]; G_loss: [ 2.42791796]\n",
      "Iter-9000; D_loss: [ 0.93306208]; G_loss: [ 2.59884763]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def G(z):\n",
    "    \"\"\"一个简单的双层神经网络作为生成器\"\"\"\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))  # @是python3中新加入的矩阵乘法符号\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    \"\"\"一个简单的双层神经网络作为生成器\"\"\"\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "# 优化器指定参数\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))\n",
    "\n",
    "\n",
    "for it in range(10000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "        # 取前16张图保存\n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "关于VAE可以直接看这篇[《Tutorial on Variational Autoencoders》](https://arxiv.org/pdf/1606.05908.pdf)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
