{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度生成模型总结--原始GAN和VAE\n",
    "\n",
    "前段时间研究了一段`Zero Shot Learning`，研究了一些生成模型，总结一下备忘。\n",
    "\n",
    "关于生成模型有一个很好的[github仓库](https://github.com/wiseodd/generative-models)，而且作者的[博客](https://wiseodd.github.io/)写的也非常精彩。\n",
    "\n",
    "所谓生成模型，即，给定一些数据，用一个向量$x$表示，每一个datapoint对应一张图片或者一句话，生成模型的目标是学会$P(x)$，有了$P(x)$就可以从中sample，从而**生成**一些数据。\n",
    "\n",
    "本文主要总结下**GAN**和**VAE**两种生成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "GAN的原始paper是大神Goodfellow的[《Generative Adversarial Nets》](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n",
    "\n",
    "先简介下模型的**思想**：\n",
    "\n",
    "GAN包含：**一个生成模型$G$**和**一个判别模型$D$**，$G$的输入是一个随机扰动$z$，输出是生成的数据$\\hat x$；$D$的输入是原始数据$x$或者$G$生成的数据$\\hat x$，输出是**输入来自训练数据的概率**。\n",
    "\n",
    "如果将$G$看作假币生产者，$D$看作警察，那么作假者的任务是尽量生产可以以假换真的假币，警察的任务是尽可能区分假币和真币，二者将不断地竞争直到生产者生产的假币完全以假乱真。\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/gan_1.png)\n",
    "\n",
    "**目标函数**：\n",
    "$$\\underset{G}{min}\\underset{D}{max}\\;V(D, G) = E_{x∼p_{data}(x)}[log D(x)] + E_{z∼p_z(z)}[log(1 − D(G(z)))]$$\n",
    "\n",
    "上面的目标函数修改自**交叉熵**损失函数。\n",
    "\n",
    "很多初次接触GAN的同学都会对GAN的训练感到迷惑，看下面的**训练算法**：\n",
    "\n",
    "- 每轮训练迭代：\n",
    "    - 前k步，训练分类器：\n",
    "        - 从先验噪声：$p_g(z)$中采样m个**噪声**作为minibatch：$\\{z^{(1)}, ..., z^{(m)}\\}$。\n",
    "        - 从数据生成分布中$p_{data}(x)$采样m个**真实数据**作为minibatch：$\\{x^{(1)}, ..., x^{(m)}\\}$。\n",
    "        - 使用随机梯度**上升**更新**判别器的参数**：\n",
    "            - $\\bigtriangledown _{\\theta_d}\\frac{1}{m}\\sum_{i=1}^m[logD(x^{(i)})+log(D(G(z^{(i)})))]$\n",
    "    - 从先验噪声：$p_g(z)$中采样m个**噪声**作为minibatch：$\\{z^{(1)}, ..., z^{(m)}\\}$。\n",
    "    - 使用随机梯度**下降**更新**生成器的参数**：\n",
    "        - $\\bigtriangledown _{\\theta_g}\\frac{1}{m}\\sum_{i=1}^mlog(D(G(z^{(i)})))$\n",
    "        \n",
    "虽然是同一个损失函数按照相反的方向训练，但训练判别器和生成器时针对的**参数**不同（上面算法中$\\theta_d$和$\\theta_g$），因此并不会出现南辕北辙的情况。\n",
    "\n",
    "**优缺点**:\n",
    "- 缺点：没有明确的$p_g(x)$的表示；$D$的训练需要和$G$同步。\n",
    "- 优点：采样时无需Markov chain；训练时无需inference，模型设计的范围更广。\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/gan_cmp.png)\n",
    "\n",
    "下面代码来自https://github.com/wiseodd/generative-models ，使用python3.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cer/anaconda2/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: [ 1.50280118]; G_loss: [ 1.64008629]\n",
      "Iter-1000; D_loss: [ 0.0023118]; G_loss: [ 8.38419437]\n",
      "Iter-2000; D_loss: [ 0.00522094]; G_loss: [ 7.93807125]\n",
      "Iter-3000; D_loss: [ 0.03023708]; G_loss: [ 4.29144192]\n",
      "Iter-4000; D_loss: [ 0.02870584]; G_loss: [ 6.23502254]\n",
      "Iter-5000; D_loss: [ 0.15886861]; G_loss: [ 4.66118479]\n",
      "Iter-6000; D_loss: [ 0.26347995]; G_loss: [ 4.20204782]\n",
      "Iter-7000; D_loss: [ 0.51319796]; G_loss: [ 2.98293495]\n",
      "Iter-8000; D_loss: [ 0.70317507]; G_loss: [ 2.42791796]\n",
      "Iter-9000; D_loss: [ 0.93306208]; G_loss: [ 2.59884763]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def G(z):\n",
    "    \"\"\"一个简单的双层神经网络作为生成器\"\"\"\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))  # @是python3中新加入的矩阵乘法符号\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    \"\"\"一个简单的双层神经网络作为生成器\"\"\"\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "# 优化器指定参数\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))\n",
    "\n",
    "\n",
    "for it in range(10000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "    D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "        # 取前16张图保存\n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成效果：\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/009_gan.png)\n",
    "\n",
    "## VAE\n",
    "\n",
    "关于VAE可以直接看这篇[《Tutorial on Variational Autoencoders》](https://arxiv.org/pdf/1606.05908.pdf)。\n",
    "\n",
    "先来看看普通的Auto-Encoder长什么样：\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/autoencoder.jpg)\n",
    "\n",
    "也就是$x$作为输入，要努力使输出$\\tilde x$接近$x$，最终使用的是中间的隐层向量，作为一个输入$x$的特征。\n",
    "\n",
    "VAE是结合了**隐变量（latent variable）**的Auto-Encoder。\n",
    "\n",
    "先考虑隐变量，假设输入的是一张猫的图片，隐变量可以是“腿的个数”/“耳朵的大小”等。\n",
    "\n",
    "那么原来的生成模型可以转换成：$P(X) = \\int P(X \\vert z) P(z) dz$，$z$是隐变量（向量）。\n",
    "\n",
    "但是手动去设计$z$向量显然是不可操作的，于是VAE假设，$z$的某个维度并不能简单地解释，并且$z\\sim N(0, I)$。即便有这样的先验假设，只要映射函数足够复杂（$P(X|z)$），最终的$P(X)$也可以足够复杂。\n",
    "\n",
    "但是这里还有一个问题，即，对于大多数的$z$，$P(X|z)=0$，直接去做积分是一个很没有必要的劳动。\n",
    "\n",
    "于是有人提出，学一个$Q(z|X)$，这个$Q$可以根据某个$X$，返回可能的$z$的分布。我们希望这里的$z$是比较可能产生$X$的，即$P(X|z)>0$，并且这里的$z$的空间比一开始$z$的空间要小。这样的话，计算$E_{z\\sim Q}P(X|z)$去代替上面的积分，会更简单。\n",
    "\n",
    "下面计算$Q(z|X)$和$P(z|X)$的**KL散度**：\n",
    "$$D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] = \\sum_z Q(z \\vert X) \\, \\log \\frac{Q(z \\vert X)}{P(z \\vert X)} \\\\\n",
    "                            = E_{z\\sim Q} \\left[ \\log \\frac{Q(z \\vert X)}{P(z \\vert X)} \\right] \\\\\n",
    "                            = E_{z\\sim Q}[\\log Q(z \\vert X) - \\log P(z \\vert X)]\\\\\n",
    "                            = E_{z\\sim Q} \\left[ \\log Q(z \\vert X) - \\log \\frac{P(X \\vert z) P(z)}{P(X)} \\right] \\\\                             = E_{z\\sim Q}[\\log Q(z \\vert X) - (\\log P(X \\vert z) + \\log P(z) - \\log P(X))] \\\\\n",
    "                            = E_{z\\sim Q}[\\log Q(z \\vert X) - \\log P(X \\vert z) - \\log P(z) + \\log P(X)]\\\\$$\n",
    "把$\\log P(X)$移到左边：\n",
    "$$\\log P(X) - D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] = E_{z\\sim Q}[\\log P(X \\vert z) - (\\log Q(z \\vert X) - \\log P(z))] \\\\\n",
    "                                       = E_{z\\sim Q}[\\log P(X \\vert z)] - E[\\log Q(z \\vert X) - \\log P(z)] \\\\\n",
    "                                       = E_{z\\sim Q}[\\log P(X \\vert z)] - D_{KL}[Q(z \\vert X) \\Vert P(z)]$$         \n",
    "\n",
    "于是我们得到了VAE的**核心方程（目标函数）**：\n",
    "$$\\log P(X) - D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)] = E_{z\\sim Q}[\\log P(X \\vert z)] - D_{KL}[Q(z \\vert X) \\Vert P(z)]$$\n",
    "\n",
    "**等式的左边**：1.$\\log P(X)$是对数似然函数，使我们要最大化的对象；2.$- D_{KL}[Q(z \\vert X) \\Vert P(z \\vert X)]$，使用$Q(z \\vert X)$代替$ P(z \\vert X)$时的产生的误差项。于是左边可以理解成：我们要找一个$\\log P(X)$的下界函数。\n",
    "\n",
    "回顾下一开始介绍的Auto-Encoder，我们可以：\n",
    "- 把$Q(z \\vert X)$当做**Encoder**；\n",
    "- 把z当做隐层；\n",
    "- 把$P(X \\vert z)$当做**Decoder**。\n",
    "\n",
    "**等式的右边**：这部分是我们可以用随机梯度下降去优化的。1.$E_{z\\sim Q}[\\log P(X \\vert z)]$是Decoder的目标函数，即如果把z理解成输入，X是输出，则$E_{z\\sim Q}[\\log P(X \\vert z)]$是极大似然函数；2.$- D_{KL}[Q(z \\vert X) \\Vert P(z)]$是Encoder的目标函数，要训练一个使$Q(z \\vert X)$尽可能接近$P(z)$的Encoder。\n",
    "\n",
    "之前我们已经限定$z\\sim N(0, I)$，现在再限定$Q(z \\vert X)$服从参数是$\\mu(X)$和$\\Sigma(X)$的高斯分布。这时，右边第二项是有准确表达式的：\n",
    "\n",
    "$$D_{KL}[N(\\mu(X), \\Sigma(X)) \\Vert N(0, 1)] = \\frac{1}{2} \\sum_k \\left( \\exp(\\Sigma(X)) + \\mu^2(X) - 1 - \\Sigma(X) \\right)$$\n",
    "\n",
    "至于右边第一个表达式，我们可以使用二次损失函数替代。\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/vae_3.png)\n",
    "\n",
    "这里为了训练的方便，使用了**重参数技巧**（reparameterization trick），即，引入一个随机性变量：$\\epsilon \\sim N(0, 1)\n",
    "$，$z = \\mu(X) + \\Sigma^{\\frac{1}{2}}(X) \\, \\epsilon$，这样使得网络反向传播时没有涉及到随机变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Iter-0; Loss: 790.1\n",
      "Iter-1000; Loss: 157.6\n",
      "Iter-2000; Loss: 132.7\n",
      "Iter-3000; Loss: 121.2\n",
      "Iter-4000; Loss: 118.5\n",
      "Iter-5000; Loss: 115.5\n",
      "Iter-6000; Loss: 118.2\n",
      "Iter-7000; Loss: 114.2\n",
      "Iter-8000; Loss: 108.9\n",
      "Iter-9000; Loss: 108.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mb_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "\n",
    "for it in range(10000):\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Forward\n",
    "    z_mu, z_var = Q(X)\n",
    "    z = sample_z(z_mu, z_var)\n",
    "    X_sample = P(z)\n",
    "\n",
    "    # Loss\n",
    "    recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "    # loss包含两项：重构损失（Decoder），kl损失（Encoder）\n",
    "    loss = recon_loss + kl_loss\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    solver.step()\n",
    "\n",
    "    # Housekeeping\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(it, loss.data[0]))\n",
    "\n",
    "        samples = P(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}_vae.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成效果：\n",
    "\n",
    "![](https://github.com/applenob/deep_learning_note/raw/master/res/009_vae.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
